---
title: CI/CD Pipeline using LangSmith Deployments and Evals
sidebarTitle: CI/CD Pipeline Example
---

This guide demonstrates how to implement a comprehensive CI/CD pipeline for AI agent applications deployed in LangSmith Deployments. In this example, we are using our open source framework [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) for orchestrating and building the agent, [LangSmith](https://docs.langchain.com/langsmith/home) for observability and evaluations. This pipeline is based on the [cicd-pipeline-example repository](https://github.com/langchain-ai/cicd-pipeline-example).

## Overview

The CI/CD pipeline provides:

- **Automated Testing**: Unit, integration, and end-to-end tests
- **Offline Evaluations**: Performance assessment using [AgentEvals](https://github.com/langchain-ai/agentevals), [OpenEvals](https://github.com/langchain-ai/openevals) and [LangSmith](https://docs.langchain.com/langsmith/home)
- **Preview and Production Deployments**: Automated staging and quality-gated production releases using the Control Plane API
- **Monitoring**: Continuous evaluation and alerting

## Prerequisites

Before setting up the CI/CD pipeline, ensure you have:

- An AI agent application (in this case built using [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview))
- [LangSmith account](/langsmith/create-account-api-key) and API key needed to deploy agents and retrieve experiment results
- Additional environment variables configured in your repository secrets

<Note>
While the example we are referring to is hosted on GitHub, this CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Pipeline Architecture

The CI/CD pipeline consists of several key components that work together to ensure code quality and reliable deployments:

```mermaid
graph TD
    A1[Code or Graph Change] --> B1[Trigger CI Pipeline]
    A2[Prompt Commit in PromptHub] --> B1
    A3[Online Evaluation Alert] --> B1
    A4[PR Opened] --> B1

    subgraph "Testing"
        B1 --> C1[Run Unit Tests]
        B1 --> C2[Run Integration Tests]
        B1 --> C3[Run End to End Tests]
        B1 --> C4[Run Offline Evaluations]

        C4 --> D1[Evaluate with OpenEvals or AgentEvals]
        C4 --> D2[Assertions: Hard and Soft]

        C1 --> E1[Run LangGraph Dev Server Test]
        C2 --> E1
        C3 --> E1
        D1 --> E1
        D2 --> E1
    end

    E1 --> F1[Push to Staging Deployment - Deploy to LangSmith as Development Type]

    F1 --> G1[Run Online Evaluations on Live Data]
    G1 --> H1[Attach Scores to Traces]

    H1 --> I1[If Quality Below Threshold]
    I1 --> J1[Send to Annotation Queue]
    I1 --> J2[Trigger Alert via Webhook]
    I1 --> J3[Push Trace to Golden Dataset]

    F1 --> K1[Promote to Production if All Pass - Deploy to LangSmith Production]

    J2 --> L1[Slack or PagerDuty Notification]

    subgraph Manual Review
        J1 --> M1[Human Labeling]
        M1 --> J3
    end
```

### Trigger Sources

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

- **Code Changes**: Pushes to main/develop branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements
- **PromptHub Updates**: Changes to prompt templates stored in LangSmith PromptHub - whenever there's a new prompt commit, we trigger a webhook to run the pipeline
- **Online Evaluation Alerts**: Performance degradation notifications from live deployments
- **LangSmith Traces Webhooks**: Automated triggers based on trace analysis and performance metrics
- **Manual Trigger**: Manual initiation of the pipeline for testing or emergency deployments

### Testing Layers

Compared to more traditional software, testing AI agent applications includes assessing the quality of the agent responses and therefore it is essential to evaluate each part of the workflow. The pipeline implements multiple testing layers:

1. **Unit Tests**: Individual node and utility function testing
2. **Integration Tests**: Component interaction testing
3. **End-to-End Tests**: Full graph execution testing
4. **Offline Evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations
5. **LangGraph Dev Server Tests**: We use our [langgraph-cli](/langgraph-platform/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. We poll the `/ok` server API endpoint until it is available and for 30 seconds, after that we throw an error

## GitHub Actions Workflow

The CI/CD pipeline leverages GitHub Actions to automate the deployment process, utilizing the [Control Plane API](/langgraph-platform/api-ref-control-plane) and [LangSmith API](/langsmith/api-reference) for seamless integration. The implementation includes a comprehensive helper script available at [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py) that facilitates API interactions and deployment management.

### New Agent Deployment

When we open a PR and tests pass, we create a new preview deployment in LangSmith Deployments using the [Control Plane API](/langgraph-platform/api-ref-control-plane). This allows us to test the agent in a staging environment before promoting to production.

### Agent Deployment Revision

![Agent Deployment Revision Workflow](./images/cicd-new-lgp-revision.png)

A revision happens when we find an existing deployment with the same ID, or when we merge the PR into main. In the case of merging to main, we delete the preview deployment and create a production deployment. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

### Testing and Evaluation Workflow

![Test with Results Workflow](./images/cicd-test-with-results.png)

In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), we have added offline evaluations and LangGraph dev server testing because we want to test the quality of our agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

## Deployment Options

We support multiple deployment methods, mostly based on the LangSmith instance you have:

- **Cloud LangSmith**: Direct GitHub integration or Docker image deployment
- **Self-Hosted/Hybrid**: Container registry-based deployments

The deployment flow starts by modifying your agent implementation. At minimum, you must have a `langgraph.json` and dependency file in your project (`requirements.txt` or `pyproject.toml`). We use the `langgraph dev` CLI tool to check for errors - if there are any, we fix the issues; otherwise, the deployment will likely succeed when deployed to LangSmith Deployments.

```mermaid
graph TD
    A[Agent Implementation] --> B[langgraph.json + dependencies]
    B --> C[Test Locally with langgraph dev]
    C --> D{Errors?}
    D -->|Yes| E[Fix Issues]
    E --> C
    D -->|No| F[Choose LangSmith Instance]

    F --> G[Cloud LangSmith]
    F --> H[Self-Hosted/Hybrid LangSmith]

    subgraph "Cloud LangSmith"
        G --> I[Method 1: Connect GitHub Repo in UI]
        G --> J[Method 2: Docker Image]
        I --> K[Deploy via LangSmith UI]
        J --> L[Build Docker Image langgraph build]
        L --> M[Push to Container Registry]
        M --> N[Deploy via Control Plane API]
    end

    subgraph "Self-Hosted/Hybrid LangSmith"
        H --> S[Build Docker Image langgraph build]
        S --> T[Push to Container Registry]
        T --> U{Deploy via?}
        U -->|UI| V[Specify Image URI in UI]
        U -->|API| W[Use Control Plane API]
        V --> X[Deploy via LangSmith UI]
        W --> Y[Deploy via Control Plane API]
    end

    K --> AA[Agent Ready for Use]
    N --> AA
    X --> AA
    Y --> AA

    AA --> BB{Connect via?}
    BB -->|LangGraph SDK| CC[Use LangGraph SDK]
    BB -->|RemoteGraph| DD[Use RemoteGraph]
    BB -->|REST API| EE[Use REST API]
    BB -->|LangGraph Studio UI| FF[Use LangGraph Studio UI]
```

### Prerequisites for Manual Deployment

Before deploying your agent, ensure you have:

1. **LangGraph Graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`)
2. **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages
3. **Configuration**: `langgraph.json` file specifying:
   - Path to your agent graph
   - Dependencies location
   - Environment variables
   - Python version

Example `langgraph.json`:
```json
{
    "graphs": {
        "simple_text2sql": "./agents/simple_text2sql.py:agent"
    },
    "env": ".env",
    "python_version": "3.11",
    "dependencies": ["."],
    "image_distro": "wolfi"
}
```

### Method 1: LangSmith Deployment UI (Cloud Only)

Deploy your agent using the LangSmith deployment interface for cloud deployments:

1. Go to your LangSmith dashboard
2. Navigate to the Deployments section
3. Connect your GitHub repository and specify the agent path

<Info>
**Benefits:**
- Simple UI-based deployment
- Direct integration with your GitHub repository
- No manual Docker image management required
</Info>

### Method 2: Build Docker Image with LangGraph CLI

Build a Docker image directly using the LangGraph CLI:

```bash
# Build Docker image
langgraph build -t my-agent:latest

# Push to your container registry
docker push my-agent:latest
```

You can push to any container registry (Docker Hub, AWS ECR, Azure ACR, Google GCR, etc.) that your deployment environment has access to.

**Deployment Options:**
- **Cloud LangSmith**: Use the Control Plane API to create deployments from your container registry
- **Self-Hosted/Hybrid LangSmith**: Choose between LangSmith UI or Control Plane API

See the [LangGraph CLI build documentation](https://docs.langchain.com/langgraph-platform/cli#build) for more details.

### Local Development & Testing

![LangGraph Studio CLI Interface](./images/cicd-studio-cli.png)

First, test your agent locally using LangGraph Studio:

```bash
# Start local development server with LangGraph Studio
langgraph dev
```

This will:
- Spin up a local server with LangGraph Studio
- Allow you to visualize and interact with your graph
- Validate that your agent works correctly before deployment

<Tip>
If your graph works in LangGraph Studio, deployment to LangGraph Platform will likely succeed.
</Tip>

See the [LangGraph CLI documentation](https://docs.langchain.com/langgraph-platform/cli#dev) for more details.

### Deploy to LangSmith

#### Cloud Deployment

![Cloud LangSmith Deployment UI](./images/cicd-cloud-lgp.png)

Deploy using the LangSmith deployment UI or the [Control Plane API](https://docs.langchain.com/langgraph-platform/api-ref-control-plane#langgraph-control-plane-api-reference):

- **UI Method**: Connect your GitHub repository directly in the LangSmith UI
- **API Method**: Use the Control Plane API to create deployments from your container registry (required for Docker images)

#### Self-Hosted/Hybrid Deployment

![Self-Hosted LangSmith Deployment UI](./images/cicd-selfhosted-lgp.png)

For [self-hosted LangSmith instances](https://docs.langchain.com/langgraph-platform/deploy-self-hosted-full-platform):

1. Ensure your Kubernetes cluster has access to your container registry
2. Build and push your Docker image to your container registry
3. Choose your deployment method:
   - **LangSmith UI**: Create a new deployment and specify your image URI (e.g., `docker.io/username/my-agent:latest`)
   - **Control Plane API**: Use the API to create deployments from your container registry

<Note>
Self-hosted deployments don't distinguish between development/production types, but you can use tags to organize them.
</Note>

See the [self-hosted full platform deployment guide](https://docs.langchain.com/langgraph-platform/deploy-self-hosted-full-platform) for detailed setup instructions.

### Connect to Your Deployed Agent

Once your agent is deployed, you can connect to it using several methods:

- **[LangGraph SDK](https://docs.langchain.com/langgraph-platform/sdk)**: Use the LangGraph SDK for programmatic integration
- **[RemoteGraph](https://docs.langchain.com/langgraph-platform/use-remote-graph)**: Connect using RemoteGraph for remote graph connections (to use your graph in other graphs)
- **[REST API](https://docs.langchain.com/langgraph-platform/server-api-ref)**: Use HTTP-based interactions with your deployed agent
- **[LangGraph Studio](https://docs.langchain.com/langgraph-platform/langgraph-studio)**: Access the visual interface for testing and debugging

### Environment Configuration

#### Database & Cache Configuration

By default, LangGraph Platform creates PostgreSQL and Redis instances for you. To use external services:

```bash
# Set environment variables for external services
export POSTGRES_URI_CUSTOM="postgresql://user:pass@host:5432/db"
export REDIS_URI_CUSTOM="redis://host:6379/0"
```

See the [environment variables documentation](https://docs.langchain.com/langgraph-platform/env-var#postgres-uri-custom) for more details.

#### Required Environment Variables

Remember to add all necessary environment variables to your deployment, including any API keys required by your specific agent implementation.

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langgraph-platform/cicd-pipeline-example.mdx)
</Callout>
